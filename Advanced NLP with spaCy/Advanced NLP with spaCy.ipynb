{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f39747f",
   "metadata": {},
   "source": [
    "# Introduction to spaCy\n",
    "\n",
    "\n",
    "The nlp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8bf4ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "Liebe Grüße!\n",
      "¿Cómo estás?\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)\n",
    "\n",
    "\n",
    "# Import the German language class\n",
    "from spacy.lang.de import German\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = German()\n",
    "\n",
    "# Process a text (this is German for: \"Kind regards!\")\n",
    "doc = nlp(\"Liebe Grüße!\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)\n",
    "\n",
    "\n",
    "# Import the Spanish language class\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = Spanish()\n",
    "\n",
    "# Process a text (this is Spanish for: \"How are you?\")\n",
    "doc = nlp(\"¿Cómo estás?\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa65dc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "tree kangaroos\n",
      "tree kangaroos and narwhals\n"
     ]
    }
   ],
   "source": [
    "## Documents, spans and tokens\n",
    "\n",
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# Select the first token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token.text)\n",
    "\n",
    "\n",
    "# part 2\n",
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos\"\n",
    "tree_kangaroos = doc[2:4]\n",
    "print(tree_kangaroos.text)\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
    "tree_kangaroos_and_narwhals = doc[2:6]\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf80d2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# Process the text\n",
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == '%':\n",
    "            print('Percentage found:', token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3974114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce36ff9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It          PRON      nsubj     \n",
      "’s          VERB      ccomp     \n",
      "official    ADJ       dobj      \n",
      ":           PUNCT     punct     \n",
      "Apple       PROPN     nsubj     \n",
      "is          AUX       ROOT      \n",
      "the         DET       det       \n",
      "first       ADJ       amod      \n",
      "U.S.        PROPN     nmod      \n",
      "public      ADJ       amod      \n",
      "company     NOUN      attr      \n",
      "to          PART      aux       \n",
      "reach       VERB      relcl     \n",
      "a           DET       det       \n",
      "$           SYM       quantmod  \n",
      "1           NUM       compound  \n",
      "trillion    NUM       nummod    \n",
      "market      NOUN      compound  \n",
      "value       NOUN      dobj      \n",
      "Apple ORG\n",
      "first ORDINAL\n",
      "U.S. GPE\n",
      "$1 trillion MONEY\n"
     ]
    }
   ],
   "source": [
    "## Predicting linguistic annotations\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print('{:<12}{:<10}{:<10}'.format(token_text, token_pos, token_dep))\n",
    "    \n",
    "\n",
    "# part 2\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84bb509e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Apple ORG\n",
      "Missing entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "## Predicting named entities in context\n",
    "\n",
    "text = \"New iPhone X release date leaked as Apple reveals pre-orders by mistake\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities\n",
    "for ent in doc.ents:\n",
    "    # print the entity text and label\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "    \n",
    "# part 2\n",
    "text = \"New iPhone X release date leaked as Apple reveals pre-orders by mistake\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities\n",
    "for ent in doc.ents:\n",
    "    # print the entity text and label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Get the span for \"iPhone X\"\n",
    "iphone_x = doc[1:3]\n",
    "\n",
    "# Print the span text\n",
    "print('Missing entity:', iphone_x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b73ae8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# part 2\n",
    "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
    "pattern = [{'TEXT': \"iPhone\"}, \n",
    "           {'TEXT': \"X\"}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('IPHONE_X_PATTERN', None, pattern)\n",
    "\n",
    "\n",
    "# part 3\n",
    "# Import the Matcher and initialize it with the shared vocabulary\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('IPHONE_X_PATTERN', None, pattern)\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print('Matches:', [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "388b6d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n",
      "Total matches found: 2\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n",
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "## Writing match patterns\n",
    "\n",
    "doc = nlp(\"After making the iOS update you won't notice a radical system-wide redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of iOS 11's furniture remains the same as in iOS 10. But you will discover some tweaks once you delve a little deeper.\")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{'TEXT': 'iOS'}, {'IS_DIGIT': True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('IOS_VERSION_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)\n",
    "    \n",
    "# <script.py> output:\n",
    "#     Total matches found: 3\n",
    "#     Match found: iOS 7\n",
    "#     Match found: iOS 11\n",
    "#     Match found: iOS 10\n",
    "    \n",
    "# part 2\n",
    "doc = nlp(\"i downloaded Fortnite on my laptop and can't open the game at all. Help? so when I was downloading Minecraft, I got the Windows version where it is the '.zip' folder and I used the default program to unpack it... do I also need to download Winzip?\")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{'LEMMA': 'download'}, {'POS': 'PROPN'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('DOWNLOAD_THINGS_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "300e8a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found: \n",
      "Match found: \n",
      "Match found: \n",
      "Match found: \n",
      "Match found: \n",
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)\n",
    "    \n",
    "# <script.py> output:\n",
    "#     Total matches found: 3\n",
    "#     Match found: iOS 7\n",
    "#     Match found: iOS 11\n",
    "#     Match found: iOS 10\n",
    "\n",
    "\n",
    "# part 3\n",
    "doc = nlp(\"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('ADJ_NOUN_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text),\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba4d87",
   "metadata": {},
   "source": [
    "# Large-scale data analysis with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd8676e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = ['spaCy', 'is', 'cool', '!']\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "596784d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "# part 2\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = ['Go', ',', 'get', 'started', '!']\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a029be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "# part 3\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Oh, really?!\"\n",
    "words = ['Oh', ',', 'really', '?', '!']\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e0fbed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n"
     ]
    }
   ],
   "source": [
    "## Docs, spans and entities from scratch\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = ['I', 'like', 'David', 'Bowie']\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adcd39ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie PERSON\n"
     ]
    }
   ],
   "source": [
    "# part 2\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label='PERSON')\n",
    "print(span.text, span.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b177bcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# part 3\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label='PERSON')\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cca99cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data structures best practices\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == 'VERB':\n",
    "            print('Found a verb after a proper noun!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce67f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tokens and part-of-speech tags\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == 'VERB':\n",
    "            print('Found a verb after a proper noun!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "808f84b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4538182773376171\n",
      "0.29217982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-142661580906>:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = doc1.similarity(doc2)\n",
      "<ipython-input-23-142661580906>:15: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = token1.similarity(token2)\n"
     ]
    }
   ],
   "source": [
    "## Comparing similarities\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)\n",
    "\n",
    "# part 2\n",
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\" \n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f142c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60264176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-b74b93e26c6f>:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = span1.similarity(span2)\n"
     ]
    }
   ],
   "source": [
    "# part 3\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96fbe895",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Debugging patterns (2)\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{'LOWER': 'amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]\n",
    "pattern2 = [{'LOWER': 'ad'}, {'TEXT': '-'}, {'LOWER': 'free'}, {'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN1', None, pattern1)\n",
    "matcher.add('PATTERN2', None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43b28a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "## Extracting countries and relationships\n",
    "\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label='GPE')\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b4c675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\" and overwrite the doc.ents\n",
    "    span = Span(doc, start, end, label='GPE')\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "    \n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, '-->', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8c3bd",
   "metadata": {},
   "source": [
    "# Processing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3677fefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f8ebf697790>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f8ebf718100>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f8ebf7180a0>)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b13c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    # Return the doc\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f15eb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    # Return the doc\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4394ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "  \n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(length_component, first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a83d898f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 3 tokens long.\n"
     ]
    }
   ],
   "source": [
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    # Return the doc\n",
    "    return doc\n",
    "  \n",
    "# Load the small English model and Add the component first in the pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(length_component, first=True)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"testetes blabla afl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f842e44a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-897eab559905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m my_text=['(sepal length: '+'{:.2f}'.format(sl)+', sepal width:'+'{:.2f}'.format(sw)+')'+\n\u001b[1;32m      2\u001b[0m   \u001b[0;34m'<br>(petal length: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'{:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m', petal width:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'{:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m')'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   for sl, sw, pl, pw in zip(list(df['sepal length (cm)']), list(df['sepal width (cm)']),\n\u001b[0m\u001b[1;32m      4\u001b[0m                            list(df['petal length (cm)']), list(df['petal width (cm)'])) ] \n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff6cdb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 8 tokens long.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    # and overwrite the doc.ents with the matched spans\n",
    "    doc.ents = [Span(doc, start, end, label='ANIMAL')\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "    \n",
    "    \n",
    "# part 2\n",
    "# Add the component to the pipeline after the 'ner' component \n",
    "nlp.add_pipe(animal_component, after='ner')\n",
    "\n",
    "\n",
    "# part 3\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "273e2260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 9 tokens long.\n",
      "has_number: True\n",
      "This document is 8 tokens long.\n",
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "## Setting extension attributes (2)\n",
    "\n",
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any((token.like_num) for token in doc)\n",
    "\n",
    "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
    "Doc.set_extension('has_number', getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute \n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print('has_number:', doc._.has_number)\n",
    "\n",
    "\n",
    "# part 2\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return '<{tag}>{text}</{tag}>'.format(tag=tag, text=span.text)\n",
    "\n",
    "# Register the Span property extension 'to_html' with the method to_html\n",
    "Span.set_extension('to_html', method=to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name 'strong'\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html('strong'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a4bdd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 26 tokens long.\n"
     ]
    }
   ],
   "source": [
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\"In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00c2074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Processing data with context\n",
    "\n",
    "# Import the Doc class and register the extensions 'author' and 'book'\n",
    "from spacy.tokens import Doc\n",
    "Doc.set_extension('book', default=None)\n",
    "Doc.set_extension('author', default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "    \n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, '\\n', \"— '{}' by {}\".format(doc._.book, doc._.author), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "\n",
    "print([token.text for token in doc])\n",
    "\n",
    "\n",
    "# part 2\n",
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered\\\n",
    "            in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    "\n",
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923818d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
